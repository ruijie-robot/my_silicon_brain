#!/usr/bin/env python3
"""
æŠ•èµ„ç ”ç©¶æ”¯æŒç³»ç»Ÿæ¼”ç¤ºè„šæœ¬
"""

import asyncio
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent / "src"))

from knowledge_base import KnowledgeBase
from local_llm_config import LocalLLMConfig


async def demo_knowledge_base():
    """æ¼”ç¤ºçŸ¥è¯†åº“åŠŸèƒ½"""
    print("\n=== çŸ¥è¯†åº“åŠŸèƒ½æ¼”ç¤º ===")
    
    kb = KnowledgeBase(collection_name="finance_knowledge_HNSW")
    
    # æ‰«ædocumentsç›®å½•
    documents_dir = "documents"
    if Path(documents_dir).exists():
        print(f"ğŸ“ æ‰«æç›®å½•: {documents_dir}")
        kb.scan_directory(documents_dir)
    else:
        print(f"âš ï¸  ç›®å½• {documents_dir} ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡æ¡£...")
    
    # æµ‹è¯•æœç´¢
    print("\nğŸ” æµ‹è¯•çŸ¥è¯†åº“æœç´¢...")
    search_queries = [
        "ä»Šå¹´å›½åº†èŠ‚æ¶ˆè´¹æ€ä¹ˆæ ·ï¼Ÿ",
        "4æœˆå…³ç¨å¯¹ä¸­å›½è‚¡å¸‚çš„å†²å‡»?",
        "é»„é‡‘æœªæ¥èµ°åŠ¿ï¼Ÿ",
        "ç‰¹æ–¯æ‹‰æœªæ¥èµ°åŠ¿ï¼Ÿ"
    ]

    for query in search_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = kb.search(query, limit=2)
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result['text'][:100]}... (ç›¸å…³åº¦: {result['score']:.3f})")



def demo_local_llm_direct():
    """æ¼”ç¤ºæœ¬åœ°LLMåŠŸèƒ½ (ç›´æ¥å‡½æ•°è°ƒç”¨æ–¹å¼)"""
    print("\n=== æœ¬åœ°LLMåŠŸèƒ½æ¼”ç¤º (ç›´æ¥è°ƒç”¨) ===")
    
    try:
        from local_llm_direct import DirectOllamaLLM
        
        direct_llm = DirectOllamaLLM()
        
        # è·å–å¯ç”¨æ¨¡å‹
        models = direct_llm.list_models()
        print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {models}")
        
        if not models:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„Ollamaæ¨¡å‹")
            print("ğŸ’¡ è¯·å…ˆå®‰è£…æ¨¡å‹: ollama pull qwen2.5:latest")
            return
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        model_name = models[1]
        embed_model_name = models[0]
        print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}, ä½¿ç”¨çš„embedæ¨¡å‹: {embed_model_name}")
        
        # æµ‹è¯•é—®é¢˜
        test_queries = [
            {
                "prompt": "ç®€è¿°ä¸­å›½è‚¡å¸‚çš„ä¸»è¦ç‰¹ç‚¹",
                "system": "ä½ æ˜¯ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ã€‚"
            },
            {
                "prompt": "è§£é‡Šä»·å€¼æŠ•èµ„çš„æ ¸å¿ƒç†å¿µ", 
                "system": "ä½ æ˜¯å·´è²ç‰¹çš„å­¦ç”Ÿï¼Œè¯·ä¼ æˆä»·å€¼æŠ•èµ„çš„ç²¾é«“ã€‚"
            },
            {
                "prompt": "æ–°èƒ½æºæ±½è½¦æ¿å—æœ‰å“ªäº›æŠ•èµ„æœºä¼šï¼Ÿ",
                "system": "ä½ æ˜¯æ–°èƒ½æºè¡Œä¸šä¸“å®¶ï¼Œè¯·åˆ†ææŠ•èµ„å‰æ™¯ã€‚"
            }
        ]
        
        # é€ä¸ªæµ‹è¯•
        for i, query_info in enumerate(test_queries, 1):
            print(f"\nğŸ“ é—®é¢˜ {i}: {query_info['prompt']}")
            print(f"ğŸ­ è§’è‰²: {query_info['system']}")
            
            response = direct_llm.simple_chat(
                model=model_name,
                prompt=query_info['prompt'],
                system_prompt=query_info['system']
            )
            
            print(f"ğŸ’¬ å›ç­”: {response[:300]}...")
            
            if i < len(test_queries):
                print("-" * 50)
        
        # æ¼”ç¤ºå¤šè½®å¯¹è¯
        print(f"\nğŸ”„ æ¼”ç¤ºå¤šè½®å¯¹è¯åŠŸèƒ½:")
        print("=" * 50)
        
        conversation_messages = [
            {'role': 'system', 'content': 'ä½ æ˜¯ä¸“ä¸šçš„è‚¡ç¥¨æŠ•èµ„é¡¾é—®'},
            {'role': 'user', 'content': 'æˆ‘æƒ³äº†è§£èŒ…å°è‚¡ç¥¨'},
        ]
        
        print("ç”¨æˆ·: æˆ‘æƒ³äº†è§£èŒ…å°è‚¡ç¥¨")
        
        response1 = direct_llm.chat(model_name, conversation_messages)
        print(f"åŠ©æ‰‹: {response1[:200]}...")
        
        # æ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
        conversation_messages.append({'role': 'assistant', 'content': response1})
        conversation_messages.append({'role': 'user', 'content': 'ç°åœ¨ä»·æ ¼åˆé€‚ä¹°å…¥å—ï¼Ÿ'})
        
        print("\nç”¨æˆ·: ç°åœ¨ä»·æ ¼åˆé€‚ä¹°å…¥å—ï¼Ÿ")
        
        response2 = direct_llm.chat(model_name, conversation_messages)
        print(f"åŠ©æ‰‹: {response2[:200]}...")
        
        # æ¼”ç¤ºæµå¼è¾“å‡º
        print(f"\nğŸŒŠ æ¼”ç¤ºæµå¼è¾“å‡º:")
        print("=" * 50)
        print("é—®é¢˜: åˆ†æä¸€ä¸‹æ¯”äºšè¿ªçš„æŠ•èµ„ä»·å€¼")
        print("å›ç­”: ", end="", flush=True)
        
        for chunk in direct_llm.stream_chat(model_name, "åˆ†æä¸€ä¸‹æ¯”äºšè¿ªçš„æŠ•èµ„ä»·å€¼ï¼ŒåŒ…æ‹¬æŠ€æœ¯ä¼˜åŠ¿å’Œå¸‚åœºå‰æ™¯"):
            print(chunk, end="", flush=True)
        print()  # æ¢è¡Œ
            
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…ollama-pythonåŒ…")
        print("ğŸ’¡ è¿è¡Œ: pip install ollama")
    except Exception as e:
        print(f"âŒ ç›´æ¥è°ƒç”¨æ¼”ç¤ºå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")




async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  æŠ•èµ„ç ”ç©¶æ”¯æŒç³»ç»Ÿæ¼”ç¤º                          â•‘
â•‘                     System Demo                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    
    # æ¼”ç¤ºå„ä¸ªåŠŸèƒ½æ¨¡å—
    try:
        await demo_knowledge_base()
    except Exception as e:
        print(f"âŒ çŸ¥è¯†åº“æ¼”ç¤ºå¤±è´¥: {e}")
    
    
    # æ¼”ç¤ºç›´æ¥è°ƒç”¨ç‰ˆæœ¬: Ruijie: done
    # try:
    #     demo_local_llm_direct()
    # except Exception as e:
    #     print(f"âŒ æœ¬åœ°LLMç›´æ¥è°ƒç”¨æ¼”ç¤ºå¤±è´¥: {e}")
    

if __name__ == "__main__":
    asyncio.run(main())