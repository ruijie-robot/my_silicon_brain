#!/usr/bin/env python3
"""
æŠ•èµ„ç ”ç©¶æ”¯æŒç³»ç»Ÿæ¼”ç¤ºè„šæœ¬
"""

import asyncio
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent / "src"))

from knowledge_base import KnowledgeBase
from local_llm_config import LocalLLMConfig


async def demo_knowledge_base():
    """æ¼”ç¤ºçŸ¥è¯†åº“åŠŸèƒ½"""
    print("\n=== çŸ¥è¯†åº“åŠŸèƒ½æ¼”ç¤º ===")
    
    kb = KnowledgeBase()
    
    # æ‰«ædocumentsç›®å½•
    documents_dir = "documents"
    if Path(documents_dir).exists():
        print(f"ğŸ“ æ‰«æç›®å½•: {documents_dir}")
        kb.scan_directory(documents_dir)
    else:
        print(f"âš ï¸  ç›®å½• {documents_dir} ä¸å­˜åœ¨ï¼Œåˆ›å»ºç¤ºä¾‹æ–‡æ¡£...")
        Path(documents_dir).mkdir(exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
        sample_content = """
# æŠ•èµ„ç­–ç•¥åˆ†æ

## ä»·å€¼æŠ•èµ„åŸåˆ™
1. å¯»æ‰¾ä½ä¼°å€¼çš„ä¼˜è´¨å…¬å¸
2. é•¿æœŸæŒæœ‰ï¼Œé¿å…é¢‘ç¹äº¤æ˜“
3. å…³æ³¨å…¬å¸åŸºæœ¬é¢å’Œè´¢åŠ¡çŠ¶å†µ
4. åˆ†æ•£æŠ•èµ„ï¼Œé™ä½é£é™©

## æŠ€æœ¯åˆ†æè¦ç‚¹
- è¶‹åŠ¿çº¿åˆ†æ
- æ”¯æ’‘ä½å’Œé˜»åŠ›ä½
- æˆäº¤é‡é…åˆ
- ç§»åŠ¨å¹³å‡çº¿ç³»ç»Ÿ

## é£é™©ç®¡ç†
- è®¾ç½®æ­¢æŸä½
- ä»“ä½æ§åˆ¶
- åˆ†æ‰¹å»ºä»“
- åŠæ—¶æ­¢ç›ˆ
"""
        
        sample_file = Path(documents_dir) / "æŠ•èµ„ç­–ç•¥.md"
        sample_file.write_text(sample_content, encoding="utf-8")
        print("âœ… åˆ›å»ºç¤ºä¾‹æ–‡æ¡£")
        
        kb.add_document(str(sample_file))
    
    # æµ‹è¯•æœç´¢
    print("\nğŸ” æµ‹è¯•çŸ¥è¯†åº“æœç´¢...")
    search_queries = [
        "ä»·å€¼æŠ•èµ„",
        "é£é™©ç®¡ç†",
        "æŠ€æœ¯åˆ†æ",
        "æŠ•èµ„ç­–ç•¥"
    ]
    
    for query in search_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = kb.search(query, limit=2)
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result['text'][:100]}... (ç›¸å…³åº¦: {result['score']:.3f})")


def demo_local_llm():
    """æ¼”ç¤ºæœ¬åœ°LLMåŠŸèƒ½ (HTTP APIæ–¹å¼)"""
    print("\n=== æœ¬åœ°LLMåŠŸèƒ½æ¼”ç¤º (HTTP API) ===")
    
    llm_config = LocalLLMConfig()
    
    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    available = llm_config.list_available_models()
    print(f"å¯ç”¨æ¨¡å‹: {available}")
    
    ollama_status = available.get("ollama", {})
    if ollama_status.get("status") == "available" and ollama_status.get("models"):
        model_name = ollama_status["models"][0]
        print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        test_queries = [
            "ç®€è¿°ä¸­å›½è‚¡å¸‚çš„ä¸»è¦ç‰¹ç‚¹",
            "è§£é‡Šä»·å€¼æŠ•èµ„çš„æ ¸å¿ƒç†å¿µ", 
            "åˆ†ææ–°èƒ½æºæ±½è½¦æ¿å—çš„æŠ•èµ„æœºä¼š"
        ]
        
        for query in test_queries:
            print(f"\né—®é¢˜: {query}")
            response = llm_config.call_ollama_model(
                model_name, 
                query,
                "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èæŠ•èµ„åˆ†æå¸ˆï¼Œè¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
            )
            print(f"å›ç­”: {response[:200]}...")
    else:
        print("âš ï¸  Ollamaä¸å¯ç”¨æˆ–æ— å¯ç”¨æ¨¡å‹")
        print("ğŸ’¡ å®‰è£…æŒ‡å—:")
        print(llm_config.setup_instructions())


def demo_local_llm_direct():
    """æ¼”ç¤ºæœ¬åœ°LLMåŠŸèƒ½ (ç›´æ¥å‡½æ•°è°ƒç”¨æ–¹å¼)"""
    print("\n=== æœ¬åœ°LLMåŠŸèƒ½æ¼”ç¤º (ç›´æ¥è°ƒç”¨) ===")
    
    try:
        from local_llm_direct import DirectOllamaLLM
        
        direct_llm = DirectOllamaLLM()
        
        # è·å–å¯ç”¨æ¨¡å‹
        models = direct_llm.list_models()
        print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {models}")
        
        if not models:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„Ollamaæ¨¡å‹")
            print("ğŸ’¡ è¯·å…ˆå®‰è£…æ¨¡å‹: ollama pull qwen2.5:latest")
            return
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        model_name = models[0]
        print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        # æµ‹è¯•é—®é¢˜
        test_queries = [
            {
                "prompt": "ç®€è¿°ä¸­å›½è‚¡å¸‚çš„ä¸»è¦ç‰¹ç‚¹",
                "system": "ä½ æ˜¯ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·ç”¨ç®€æ´çš„è¯­è¨€å›ç­”ã€‚"
            },
            {
                "prompt": "è§£é‡Šä»·å€¼æŠ•èµ„çš„æ ¸å¿ƒç†å¿µ", 
                "system": "ä½ æ˜¯å·´è²ç‰¹çš„å­¦ç”Ÿï¼Œè¯·ä¼ æˆä»·å€¼æŠ•èµ„çš„ç²¾é«“ã€‚"
            },
            {
                "prompt": "æ–°èƒ½æºæ±½è½¦æ¿å—æœ‰å“ªäº›æŠ•èµ„æœºä¼šï¼Ÿ",
                "system": "ä½ æ˜¯æ–°èƒ½æºè¡Œä¸šä¸“å®¶ï¼Œè¯·åˆ†ææŠ•èµ„å‰æ™¯ã€‚"
            }
        ]
        
        # é€ä¸ªæµ‹è¯•
        for i, query_info in enumerate(test_queries, 1):
            print(f"\nğŸ“ é—®é¢˜ {i}: {query_info['prompt']}")
            print(f"ğŸ­ è§’è‰²: {query_info['system']}")
            
            response = direct_llm.simple_chat(
                model=model_name,
                prompt=query_info['prompt'],
                system_prompt=query_info['system']
            )
            
            print(f"ğŸ’¬ å›ç­”: {response[:300]}...")
            
            if i < len(test_queries):
                print("-" * 50)
        
        # æ¼”ç¤ºå¤šè½®å¯¹è¯
        print(f"\nğŸ”„ æ¼”ç¤ºå¤šè½®å¯¹è¯åŠŸèƒ½:")
        print("=" * 50)
        
        conversation_messages = [
            {'role': 'system', 'content': 'ä½ æ˜¯ä¸“ä¸šçš„è‚¡ç¥¨æŠ•èµ„é¡¾é—®'},
            {'role': 'user', 'content': 'æˆ‘æƒ³äº†è§£èŒ…å°è‚¡ç¥¨'},
        ]
        
        print("ç”¨æˆ·: æˆ‘æƒ³äº†è§£èŒ…å°è‚¡ç¥¨")
        
        response1 = direct_llm.chat(model_name, conversation_messages)
        print(f"åŠ©æ‰‹: {response1[:200]}...")
        
        # æ·»åŠ AIå›å¤åˆ°å¯¹è¯å†å²
        conversation_messages.append({'role': 'assistant', 'content': response1})
        conversation_messages.append({'role': 'user', 'content': 'ç°åœ¨ä»·æ ¼åˆé€‚ä¹°å…¥å—ï¼Ÿ'})
        
        print("\nç”¨æˆ·: ç°åœ¨ä»·æ ¼åˆé€‚ä¹°å…¥å—ï¼Ÿ")
        
        response2 = direct_llm.chat(model_name, conversation_messages)
        print(f"åŠ©æ‰‹: {response2[:200]}...")
        
        # æ¼”ç¤ºæµå¼è¾“å‡º
        print(f"\nğŸŒŠ æ¼”ç¤ºæµå¼è¾“å‡º:")
        print("=" * 50)
        print("é—®é¢˜: åˆ†æä¸€ä¸‹æ¯”äºšè¿ªçš„æŠ•èµ„ä»·å€¼")
        print("å›ç­”: ", end="", flush=True)
        
        for chunk in direct_llm.stream_chat(model_name, "åˆ†æä¸€ä¸‹æ¯”äºšè¿ªçš„æŠ•èµ„ä»·å€¼ï¼ŒåŒ…æ‹¬æŠ€æœ¯ä¼˜åŠ¿å’Œå¸‚åœºå‰æ™¯"):
            print(chunk, end="", flush=True)
        print()  # æ¢è¡Œ
            
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…ollama-pythonåŒ…")
        print("ğŸ’¡ è¿è¡Œ: pip install ollama")
    except Exception as e:
        print(f"âŒ ç›´æ¥è°ƒç”¨æ¼”ç¤ºå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")


async def demo_web_search():
    """æ¼”ç¤ºç½‘ç»œæœç´¢åŠŸèƒ½"""
    print("\n=== ç½‘ç»œæœç´¢åŠŸèƒ½æ¼”ç¤º ===")
    
    try:
        from mcp_web_search import WebSearcher
        
        searcher = WebSearcher()
        
        # æµ‹è¯•æœç´¢åŠŸèƒ½
        print("ğŸŒ æµ‹è¯•DuckDuckGoæœç´¢...")
        results = searcher.search_duckduckgo("æ–°èƒ½æºæ±½è½¦ è‚¡ç¥¨", 3)
        
        for i, result in enumerate(results, 1):
            print(f"\n{i}. {result['title']}")
            print(f"   URL: {result['url']}")
            print(f"   æ‘˜è¦: {result['snippet'][:100]}...")
        
        # æµ‹è¯•é‡‘èæ–°é—»
        print("\nğŸ“° æµ‹è¯•é‡‘èæ–°é—»è·å–...")
        news = searcher.search_finance_news("æ–°èƒ½æº")
        
        for i, item in enumerate(news[:3], 1):
            print(f"\n{i}. {item['title']}")
            print(f"   æ¥æº: {item['source']}")
            print(f"   æ‘˜è¦: {item['summary'][:100]}...")
            
    except Exception as e:
        print(f"âŒ ç½‘ç»œæœç´¢æ¼”ç¤ºå¤±è´¥: {e}")
        print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–åŒ…")


def demo_file_structure():
    """æ˜¾ç¤ºé¡¹ç›®æ–‡ä»¶ç»“æ„"""
    print("\n=== é¡¹ç›®æ–‡ä»¶ç»“æ„ ===")
    
    def print_tree(directory, prefix="", level=0):
        if level > 2:  # é™åˆ¶æ˜¾ç¤ºå±‚çº§
            return
            
        path = Path(directory)
        if not path.exists():
            return
            
        items = sorted(path.iterdir(), key=lambda x: (x.is_file(), x.name))
        
        for i, item in enumerate(items):
            is_last = i == len(items) - 1
            current_prefix = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            print(f"{prefix}{current_prefix}{item.name}")
            
            if item.is_dir() and not item.name.startswith('.'):
                extension = "    " if is_last else "â”‚   "
                print_tree(item, prefix + extension, level + 1)
    
    print("ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„:")
    print_tree(".")


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  æŠ•èµ„ç ”ç©¶æ”¯æŒç³»ç»Ÿæ¼”ç¤º                          â•‘
â•‘                     System Demo                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # æ˜¾ç¤ºé¡¹ç›®ç»“æ„
    demo_file_structure()
    
    # æ¼”ç¤ºå„ä¸ªåŠŸèƒ½æ¨¡å—
    try:
        await demo_knowledge_base()
    except Exception as e:
        print(f"âŒ çŸ¥è¯†åº“æ¼”ç¤ºå¤±è´¥: {e}")
    
    try:
        demo_local_llm()
    except Exception as e:
        print(f"âŒ æœ¬åœ°LLMæ¼”ç¤ºå¤±è´¥: {e}")
    
    # æ¼”ç¤ºç›´æ¥è°ƒç”¨ç‰ˆæœ¬
    try:
        demo_local_llm_direct()
    except Exception as e:
        print(f"âŒ æœ¬åœ°LLMç›´æ¥è°ƒç”¨æ¼”ç¤ºå¤±è´¥: {e}")
    
    try:
        await demo_web_search()
    except Exception as e:
        print(f"âŒ ç½‘ç»œæœç´¢æ¼”ç¤ºå¤±è´¥: {e}")
    
    print("""
ğŸ“‹ æ¼”ç¤ºå®Œæˆï¼

æ¥ä¸‹æ¥çš„æ­¥éª¤:
1. é…ç½®APIå¯†é’¥ (ç¼–è¾‘.envæ–‡ä»¶)
2. å®‰è£…æœ¬åœ°LLMæ¨¡å‹ (å¯é€‰)
   - è¿è¡Œ: python src/local_llm_config.py
3. æ·»åŠ æ‚¨çš„æŠ•èµ„æ–‡æ¡£åˆ°documentsç›®å½•
4. è¿è¡Œå®Œæ•´ç³»ç»Ÿ: python main.py

æ›´å¤šå¸®åŠ©: python main.py --help
    """)


if __name__ == "__main__":
    asyncio.run(main())