import ollama
from ollama import chat
from ollama import ChatResponse

# single = ollama.embed(
#   model='qwen3-embedding:0.6b',
#   input='The quick brown fox jumps over the lazy dog.'
# )
# print(len(single['embeddings'][0]))  # vector length

# response: ChatResponse = chat(model='qwen3:0.6b', messages=[
#   {
#     'role': 'user',
#     'content': 'Why is the sky blue?',
#   },
# ])
# print(response['message']['content'])
# # or access fields directly from the response object
# print(response.message.content)

####### streaming
stream = chat(
  model='qwen3:0.6b',
  messages=[{'role': 'user', 'content': 'What is 17 Ã— 23?'}],
  stream=True,
)

in_thinking = False
content = ''
thinking = ''
for chunk in stream:
  if chunk.message.thinking:
    if not in_thinking:
      in_thinking = True
      print('Thinking:\n', end='', flush=True)
    print(chunk.message.thinking, end='', flush=True)
    # accumulate the partial thinking 
    thinking += chunk.message.thinking
  elif chunk.message.content:
    if in_thinking:
      in_thinking = False
      print('\n\nAnswer:\n', end='', flush=True)
    print(chunk.message.content, end='', flush=True)
    # accumulate the partial content
    content += chunk.message.content

  # append the accumulated fields to the messages for the next request
  new_messages = [{ "role": 'assistant', "thinking": thinking, "content": content }]

print("Finished")